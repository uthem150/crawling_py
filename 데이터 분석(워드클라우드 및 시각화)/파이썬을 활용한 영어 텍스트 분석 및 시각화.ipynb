{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  영문 텍스트 분석 후 시각화 하기\n",
    "\n",
    "import nltk\n",
    "nltk.download( )   # 이 작업은 시간이 몇 분에서 몇십분 정도 소요됩니다.\n",
    "\n",
    "# nltk 패키지가 잘 설치되었는지 테스트하기\n",
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f5283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 분석할 예제 텍스트를 불러옵니다\n",
    "data1 = open(\"c:\\\\temp\\\\fruits.txt\").readlines( )\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bcbb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 텍스트를 토큰화하기 - 단어별로 구분화하는 작업\n",
    "data1_2=[]\n",
    "for a in data1 :\n",
    "    data1_2.append(nltk.word_tokenize(a) )\n",
    "print(data1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_tokenize( ) 함수는 축약형을 제대로 분리할 수 없음\n",
    "# 그래서 아래의 함수를 추천함\n",
    "from nltk.tokenize  import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer( )\n",
    "\n",
    "data1_3=[]\n",
    "for a in data1 :\n",
    "    data1_3.append( tokenizer.tokenize(a) )\n",
    "print(data1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04550d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 축약형의 경우 축약된 단어를 하나의 단어로 인식시키고 싶을 경우\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer( \"[\\w']+\") \n",
    "\n",
    "data1_4=[]\n",
    "for a in data1 :\n",
    "    data1_4.append( tokenizer.tokenize(a) )\n",
    "print(data1_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 리스트별로 중복값 제거하기\n",
    "#set( ) 자료형은 내부에 중복된 값을 허용하지 않음\n",
    "data1_5= []\n",
    "for b in data1_4 :\n",
    "    data1_5.append( list(set(b) ) )\n",
    "print(data1_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fed2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 리스트로 되어 있는 텍스트를 1개의 벡터로 변환\n",
    "data1_6=[ ]\n",
    "n_row = len(data1_5)\n",
    "\n",
    "for c in range(0,n_row):\n",
    "    for d in range(0,len(data1_5[c])) :\n",
    "        data1_6.append( data1_5[c][d])\n",
    "print(data1_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c5081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 기본 제공되는 stopwords 사전으로 불용어 제거하기\n",
    "from nltk.corpus import stopwords\n",
    "data3 = [ each_word for each_word in data1_6\n",
    "          if each_word not in stopwords.words() ]\n",
    "print(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8a77d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list 내포 for 반복문\n",
    "# 문법 :\n",
    "# [ 표현식 for 항목in 반복할데이터 if조건문 ]\n",
    "no = [1,2,3]\n",
    "result = [ i*10 for i in no]\n",
    "print(result)\n",
    "\n",
    "#no2 값 중 짝수만 제곱\n",
    "no2 = [1,2,3,4,5,6]\n",
    "result2 = [ i*i for i in no2  if i %2==0 ]\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c92501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자가 생성한 불용어 사전을 불러와서 불용어를 제거합니다.\n",
    "sword2 = open(\"c:\\\\temp\\\\stopword_list.txt\").read()\n",
    "print(sword2)\n",
    "data4 = [ each_word for each_word in data3\n",
    "          if each_word not in sword2 ]\n",
    "print(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자수로 불용어 제거하기\n",
    "data5 = []\n",
    "for i in data4 :\n",
    "    if len(i) >= 2 and len(i) <= 10 :\n",
    "        data5.append(i) \n",
    "print(data5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별로 언급 빈도 조사합니다\n",
    "from collections import Counter\n",
    "data6 = Counter(data5)\n",
    "print(data6)\n",
    "data7 = data6.most_common(10)\n",
    "print(data7)\n",
    "data8 = dict(data7)\n",
    "print(data8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드 클라우드로 시각화 하기\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(font_path='c:\\\\windows\\\\fonts\\HMKMG.TTF' ,\n",
    "                      relative_scaling = 0.5 ,\n",
    "                      background_color='white' ).generate_from_frequencies(data8)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8782cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어별 언급 빈도로 그래프 그리기\n",
    "graph = nltk.Text(data5 , name ='각 단어별 언급빈도수')\n",
    "graph.plot(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c17c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c4434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lesson 3.연설문 분석\n",
    "steve = open(\"c:\\\\temp\\\\steve.txt\").read()\n",
    "print(steve)\n",
    "\n",
    "#word_tokenize( ) 함수는 축약형을 제대로 분리할 수 없음\n",
    "# 그래서 아래의 함수를 추천함\n",
    "from nltk.tokenize  import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer( )\n",
    "data1 = tokenizer.tokenize(steve)\n",
    "print(data1)\n",
    "\n",
    "# 불용어 제거하기\n",
    "from nltk.corpus import stopwords\n",
    "data1 = [ each_word for each_word in data1\n",
    "          if each_word not in stopwords.words() ]\n",
    "print(data1)\n",
    "\n",
    "# 사용자가 생성한 불용어 사전을 불러와서 불용어를 제거합니다.\n",
    "sword2 = open(\"c:\\\\temp\\\\steve_stopword_list.txt\").read()\n",
    "\n",
    "data1 = [ each_word for each_word in data1\n",
    "          if each_word not in sword2 ]\n",
    "print(data1)\n",
    "\n",
    "# 글자수로 불용어 제거하기\n",
    "data2 = []\n",
    "for i in data1 :\n",
    "    if len(i) >= 2 and len(i) <= 10 :\n",
    "        data2.append(i) \n",
    "print(data2)\n",
    "\n",
    "# 단어별로 언급 빈도 조사합니다\n",
    "from collections import Counter\n",
    "data3 = Counter(data2)\n",
    "print(data3)\n",
    "data4 = data3.most_common(100)\n",
    "print(data4)\n",
    "data5 = dict(data4)\n",
    "print(data5)\n",
    "\n",
    "# 워드 클라우드로 시각화 하기\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(font_path='c:\\\\windows\\\\fonts\\HMKMG.TTF' ,\n",
    "                      relative_scaling = 0.5 ,\n",
    "                      background_color='white' ).generate_from_frequencies(data5)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4cb410",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np         # pip install numpy\n",
    "from PIL import Image      # pip install Image\n",
    "from wordcloud import ImageColorGenerator\n",
    "apple = np.array(Image.open(\"c:\\\\temp\\\\image\\\\apple.png\"))\n",
    "wc = WordCloud(font_path=\"c:\\\\windows\\\\fonts\\\\HMKMG.TTF\" ,\n",
    "                       relative_scaling=0.2,mask = apple,\n",
    "                       background_color=\"white\",\n",
    "                       min_font_size=1,\n",
    "                       max_font_size=50,\n",
    "                       max_words=500\n",
    "                     ).generate_from_frequencies(data5)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어별 언급 빈도로 그래프 그리기\n",
    "graph = nltk.Text(data2 , name ='각 단어별 언급빈도수')\n",
    "graph.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b2b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
