{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chap 15.riss.kr 에서 특정 키워드로 논문 / 학술 자료 검색하기\n",
    "\n",
    "#Step 1. 필요한 모듈을 로딩합니다\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time \n",
    "\n",
    "#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.\n",
    "print(\"=\" *100)\n",
    "print(\" 이 크롤러는 RISS 사이트의 논문 및 학술자료 수집용 웹크롤러입니다.\")\n",
    "print(\"=\" *100)\n",
    "query_txt = input('1.수집할 자료의 키워드는 무엇입니까?: ')\n",
    "\n",
    "#Step 3. 수집된 데이터를 저장할 파일 이름 입력받기 \n",
    "f_dir = input(\"2.파일을 저장할 폴더명만 쓰세요(기본값:c:\\\\py_temp\\\\):\")\n",
    "if f_dir == '' :\n",
    "    f_dir=\"c:\\\\py_temp\\\\\"\n",
    "\n",
    "#Step 4. 크롬 드라이버 설정 및 웹 페이지 열기\n",
    "s = Service(\"c:/py_temp/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'http://www.riss.kr/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step 5. 자동으로 검색어 입력 후 조회하기\n",
    "element = driver.find_element(By.ID,'query')\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys(\"\\n\")\n",
    "\n",
    "#Step 6.학위 논문 선택하기\n",
    "driver.find_element(By.LINK_TEXT,'학위논문').click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step 7.Beautiful Soup 로 본문 내용만 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "html_1 = driver.page_source\n",
    "soup_1 = BeautifulSoup(html_1, 'html.parser')\n",
    "\n",
    "#Step 8. 총 검색 건수를 보여주고 수집할 건수 입력받기\n",
    "import math\n",
    "total_cnt = soup_1.find('div','searchBox pd').find('span','num').get_text()\n",
    "print('검색하신 키워드 %s (으)로 총 %s 건의 학위논문이 검색되었습니다' %(query_txt,total_cnt))\n",
    "cnt = int(input('이 중에서 몇 건을 수집하시겠습니까?: '))\n",
    "page_cnt = math.ceil(cnt / 10)\n",
    "print('%s 건의 데이터를 수집하기 위해 %s 페이지의 게시물을 조회합니다.' %(cnt,page_cnt))\n",
    "print(\"\\n\")\n",
    "\n",
    "#Step 9. 데이터 수집하기\n",
    "no2=[]           # 게시글 번호 컬럼\n",
    "title2=[ ]       # 게시글 제목 컬럼\n",
    "author2=[]       # 논문 저자 컬럼\n",
    "company2=[ ]     # 소속 기관 컬럼\n",
    "date2=[ ]        # 게시글 날짜 컬럼\n",
    "suksa2=[ ]       # 국내석사 컬럼\n",
    "contents2=[]     # 초록내용\n",
    "full_url2=[]     # 논문 원본 URL\n",
    "\n",
    "no = 1           # 게시글 번호 초기값\n",
    "            \n",
    "for a in range(1,page_cnt+1) :\n",
    "    print(\"\\n\")\n",
    "    print(\"%s 페이지 내용 수집 시작합니다 =======================\" %a)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content_list = soup.find('div','srchResultListW').find_all('li')\n",
    "\n",
    "    for i in content_list:\n",
    "        # 논문 제목 체크하기\n",
    "        try:\n",
    "            title=i.find('p','title').get_text().strip()\n",
    "        except :\n",
    "            continue \n",
    "        else :\n",
    "            # 1.게시글 번호\n",
    "            print(\"\\n\")\n",
    "            print(\"%s 번째 정보를 추출하고 있습니다============\" %no)\n",
    "            no2.append(no)\n",
    "            print(\"1.번호 : %s\" %no)\n",
    "            \n",
    "            # 2. 논문 제목\n",
    "            title2.append(title.strip())\n",
    "            print(\"2.제목 : %s\" %title.strip())\n",
    "\n",
    "            # 3. 작성자\n",
    "            try :\n",
    "                author=i.find('p','etc').find('span','writer').get_text().strip()\n",
    "            except :\n",
    "                author = '작성자가 없습니다'\n",
    "                print(\"3.작성자 : %s\" %author.strip())\n",
    "                author2.append(author.strip())\n",
    "            else :\n",
    "                author2.append(author.strip())\n",
    "                print(\"3.작성자 : %s\" %author.strip())\n",
    "\n",
    "            # 4. 소속기관\n",
    "            try :\n",
    "                company=i.find('p','etc').find('span','assigned').get_text().strip()\n",
    "            except :\n",
    "                company='소속 기관이 없습니다'\n",
    "                company2.append(company.strip())\n",
    "                print(\"4.소속기관 : %s\" %company.strip())\n",
    "            else :\n",
    "                company2.append(company.strip())\n",
    "                print(\"4.소속기관 : %s\" %company.strip())\n",
    "\n",
    "            # 5. 발표날짜\n",
    "            try :\n",
    "                date_1 =i.find('p','etc').find_all('span')\n",
    "                date_2 = date_1[2].get_text().strip()\n",
    "            except :\n",
    "                date_2='발표날짜가 없습니다'\n",
    "                date2.append(date_2)\n",
    "                print(\"5.발표년도 : %s\" %date_2)\n",
    "            else :\n",
    "                date2.append(date_2)\n",
    "                print(\"5.발표년도 : %s\" %date_2)\n",
    "\n",
    "            # 6.학위여부\n",
    "            try :\n",
    "                suksa_1 =i.find('p','etc').find_all('span')\n",
    "                suksa_2 = suksa_1[3].get_text().strip()\n",
    "            except :\n",
    "                suksa_2='학위가 없습니다'\n",
    "                suksa2.append(suksa_2)\n",
    "                print(\"6.학위여부 : %s\" %suksa_2)\n",
    "            else :\n",
    "                suksa2.append(suksa_2)\n",
    "                print(\"6.학위여부 : %s\" %suksa_2)\n",
    "\n",
    "            # 7.초록 내용-해당 논문의 상세 내역에서 추출할 수 있음.    \n",
    "            url_1 = i.find('p','title').find('a')['href']\n",
    "            full_url = 'http://www.riss.kr'+url_1\n",
    "            time.sleep(1)\n",
    "            driver.get(full_url)\n",
    "\n",
    "            html_1 = driver.page_source\n",
    "            soup_1 = BeautifulSoup(html_1, 'html.parser')  \n",
    "            try :\n",
    "                cont=soup_1.find(\"div\",\"text\").find('p').get_text().replace(\"\\n\",\"\").strip()\n",
    "            except :\n",
    "                cont='초록이 없습니다'\n",
    "                contents2.append(cont)\n",
    "                print(\"7.초록내용 : %s\" %cont)\n",
    "            else :\n",
    "                contents2.append(cont)\n",
    "                print(\"7.초록내용 : %s\" %cont)\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            # 8.논문 url 주소\n",
    "            full_url2.append(full_url)\n",
    "            print('8.논문 URL 주소:' , full_url)\n",
    "\n",
    "            driver.back()  # 이전 페이지로 돌아가기\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            no += 1\n",
    "            \n",
    "            if no > cnt :\n",
    "                break \n",
    "                            \n",
    "    a += 1 \n",
    "    b = str(a)\n",
    "\n",
    "    try :\n",
    "        driver.find_element(By.LINK_TEXT ,'%s' %b).click() \n",
    "    except :\n",
    "        driver.find_element(By.LINK_TEXT,'다음 페이지로').click()\n",
    "        \n",
    "print(\"요청하신 작업이 모두 완료되었습니다\")\n",
    "\n",
    "# Step 10. 수집된 데이터를 xls와 csv 형태로 저장하기\n",
    "# 현재 날짜와 시간으로 폴더 만들고 파일 이름 설정하기\n",
    "import os\n",
    "\n",
    "n = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' %(n.tm_year, n.tm_mon, n.tm_mday, n.tm_hour, n.tm_min, n.tm_sec)\n",
    "\n",
    "os.makedirs(f_dir+'RISS'+'-'+s+'-'+'학위논문')\n",
    "\n",
    "fc_name = f_dir+'RISS'+'-'+s+'-'+'학위논문'+'\\\\'+'RISS'+'-'+s+'-'+'학위논문'+'.csv'\n",
    "fx_name = f_dir+'RISS'+'-'+s+'-'+'학위논문'+'\\\\'+'RISS'+'-'+s+'-'+'학위논문'+'.xls'\n",
    "\n",
    "# 데이터 프레임 생성 후 xls , csv 형식으로 저장하기\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['번호']=no2\n",
    "df['제목']=pd.Series(title2)\n",
    "df['저자']=pd.Series(author2)\n",
    "df['소속(발행)기관']=pd.Series(company2)\n",
    "df['날짜']=pd.Series(date2)\n",
    "df['학위(논문일경우)']=pd.Series(suksa2)\n",
    "df['초록(논문일경우)']=pd.Series(contents2)\n",
    "df['자료URL주소']=pd.Series(full_url2)\n",
    "\n",
    "# xls 형태로 저장하기\n",
    "df.to_excel(fx_name,index=False, encoding=\"utf-8\" , engine='openpyxl')\n",
    "\n",
    "# csv 형태로 저장하기\n",
    "df.to_csv(fc_name,index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
